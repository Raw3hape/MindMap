//
//  OpenAIService.swift
//  MindMap
//
//  Created by Nikita Sergyshkin on 05/08/2025.
//

import Foundation
import AVFoundation

// MARK: - OpenAI Response Models
struct OpenAITaskResponse: Codable {
    let title: String
    let description: String?
    let priority: String
    let subtasks: [String]
    let originalText: String
}

struct ProcessAudioRequest: Codable {
    let audioData: String // Base64 encoded audio
    let mimeType: String
}

struct ProcessAudioResponse: Codable {
    let success: Bool
    let data: OpenAITaskResponse?
    let error: String?
}

// MARK: - Processing Mode
enum ProcessingMode: CaseIterable {
    case speechOnly       // –¢–æ–ª—å–∫–æ iOS Speech Framework + —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    case auto             // –£–º–Ω—ã–π –≤—ã–±–æ—Ä (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ Speech)
    
    var displayName: String {
        switch self {
        case .speechOnly:
            return "iOS Speech + GPT"
        case .auto:
            return "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π"
        }
    }
    
    var description: String {
        switch self {
        case .speechOnly:
            return "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç iOS Speech Framework + GPT –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"
        case .auto:
            return "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –º–µ—Ç–æ–¥"
        }
    }
}

// MARK: - OpenAI Service
@MainActor
class OpenAIService: ObservableObject {
    static let shared = OpenAIService()
    
    @Published var processingMode: ProcessingMode = .speechOnly
    @Published var useOfflineFirst: Bool = true
    
    private let baseURL = "https://mind-map-tau-roan.vercel.app" // Production URL - —Ä–∞–±–æ—Ç–∞–µ—Ç!
    private let session = URLSession.shared
    private let speechService = SpeechRecognitionService.shared
    private let localAnalyzer = LocalTaskAnalyzer.shared
    
    private init() {}
    
    // MARK: - Process Audio with Hybrid Approach
    func processAudio(from audioURL: URL) async throws -> Task {
        return try await processAudioWithMode(from: audioURL, mode: processingMode)
    }
    
    func processAudioWithMode(from audioURL: URL, mode: ProcessingMode) async throws -> Task {
        logInfo("üé§ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∂–∏–º–æ–º: \(mode)", category: .audio)
        
        // –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º iOS Speech Framework + GPT –∞–Ω–∞–ª–∏–∑
        switch mode {
        case .speechOnly, .auto:
            return try await processAudioWithSpeechFramework(from: audioURL)
        }
    }
    
    // MARK: - Speech Framework Processing
    private func processAudioWithSpeechFramework(from audioURL: URL) async throws -> Task {
        logInfo("üó£Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º iOS Speech Framework", category: .speech)
        
        let transcription = try await speechService.transcribeAudioFile(at: audioURL)
        logInfo("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞: \(transcription.prefix(50))...", category: .speech)
        
        // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ API
        return try await processTextViaAPI(transcription, audioURL: audioURL)
    }
    
    
    
    // MARK: - Process Text
    func processText(_ text: String) async throws -> Task {
        return try await processTextViaAPI(text, audioURL: nil)
    }
    
    // MARK: - API Text Processing with Fallback
    private func processTextViaAPI(_ text: String, audioURL: URL?) async throws -> Task {
        // –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º API
        do {
            logInfo("üåê –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ API", category: .network)
            return try await processTextWithAPI(text, audioURL: audioURL)
        } catch {
            // –ü—Ä–∏ –æ—à–∏–±–∫–µ API –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            logInfo("üì± API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä", category: .network) 
            let localTask = localAnalyzer.analyzeText(text)
            
            // –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É –µ—Å–ª–∏ –µ—Å—Ç—å
            if let audioURL = audioURL {
                return Task(
                    title: localTask.title,
                    description: localTask.description,
                    priority: localTask.priority,
                    audioFilePath: audioURL.path,
                    originalText: text,
                    subtasks: localTask.subtasks
                )
            } else {
                return localTask
            }
        }
    }
    
    // MARK: - API Text Processing
    private func processTextWithAPI(_ text: String, audioURL: URL?) async throws -> Task {
        let requestBody = ["text": text]
        let fullURL = "\(baseURL)/api/process-text"
        
        guard let url = URL(string: fullURL) else {
            throw OpenAIError.invalidURL
        }
        
        var urlRequest = URLRequest(url: url)
        urlRequest.httpMethod = "POST"
        urlRequest.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        do {
            urlRequest.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
        } catch {
            throw OpenAIError.encodingError
        }
        
        let (data, response) = try await session.data(for: urlRequest)
        
        guard let httpResponse = response as? HTTPURLResponse else {
            throw OpenAIError.invalidResponse
        }
        
        logInfo("üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç API: —Å—Ç–∞—Ç—É—Å \(httpResponse.statusCode)", category: .network)
        
        if httpResponse.statusCode != 200 {
            let errorData = String(data: data, encoding: .utf8) ?? "No error data"
            logError("‚ùå –û—à–∏–±–∫–∞ API \(httpResponse.statusCode): \(errorData)", category: .network)
            throw OpenAIError.serverError(httpResponse.statusCode)
        }
        
        let responseString = String(data: data, encoding: .utf8) ?? "No data"
        logInfo("üì¶ –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç API: \(responseString)", category: .network)
        
        do {
            let processResponse = try JSONDecoder().decode(ProcessAudioResponse.self, from: data)
            logInfo("‚úÖ –£—Å–ø–µ—à–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç: \(processResponse.success)", category: .network)
            
            if processResponse.success, let taskData = processResponse.data {
                let task = convertToTask(from: taskData, audioURL: audioURL)
                logInfo("üéØ –°–æ–∑–¥–∞–Ω–∞ –∑–∞–¥–∞—á–∞ —á–µ—Ä–µ–∑ API: \(task.title)", category: .network)
                return task
            } else {
                logError("‚ùå API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: \(processResponse.error ?? "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")", category: .network)
                throw OpenAIError.processingError(processResponse.error ?? "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
            }
        } catch {
            logError("‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: \(error)", category: .network)
            throw OpenAIError.decodingError
        }
    }
    
    // MARK: - Configuration Methods
    func setProcessingMode(_ mode: ProcessingMode) {
        processingMode = mode
        logInfo("‚öôÔ∏è –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: \(mode)", category: .audio)
    }
    
    func setOfflineFirst(_ enabled: Bool) {
        useOfflineFirst = enabled
        logInfo("üì± –û—Ñ–ª–∞–π–Ω-–ø–µ—Ä–≤—ã–π —Ä–µ–∂–∏–º: \(enabled ? "–≤–∫–ª—é—á–µ–Ω" : "–≤—ã–∫–ª—é—á–µ–Ω")", category: .audio)
    }
    
    func getSpeechRecognitionAvailability() -> Bool {
        return speechService.isSpeechRecognitionAvailable
    }
    
    func getRecommendedMode(for audioURL: URL) -> ProcessingMode {
        // –í—Å–µ–≥–¥–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º iOS Speech + GPT –∞–Ω–∞–ª–∏–∑
        return .speechOnly
    }
    
    // MARK: - Helper Methods
    private func convertToTask(from response: OpenAITaskResponse, audioURL: URL?) -> Task {
        let priority = mapPriority(from: response.priority)
        
        let subtasks = response.subtasks.map { title in
            Subtask(title: title)
        }
        
        return Task(
            title: response.title,
            description: response.description,
            priority: priority,
            audioFilePath: audioURL?.path,
            originalText: response.originalText,
            subtasks: subtasks
        )
    }
    
    private func mapPriority(from priorityString: String) -> TaskPriority {
        let lowercased = priorityString.lowercased()
        
        // –ú–∞–ø–ø–∏–Ω–≥ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç OpenAI
        switch lowercased {
        case "low", "–Ω–∏–∑–∫–∏–π", "1":
            return .low
        case "high", "urgent", "–≤–∞–∂–Ω—ã–π", "–≤—ã—Å–æ–∫–∏–π", "3":
            return .high
        case "medium", "normal", "—Å—Ä–µ–¥–Ω–∏–π", "2":
            return .medium
        default:
            return .medium
        }
    }
}

// MARK: - OpenAI Errors
enum OpenAIError: LocalizedError {
    case invalidURL
    case encodingError
    case decodingError
    case invalidResponse
    case serverError(Int)
    case processingError(String)
    case networkError(String)
    
    var errorDescription: String? {
        switch self {
        case .invalidURL:
            return "–ù–µ–≤–µ—Ä–Ω—ã–π URL"
        case .encodingError:
            return "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"
        case .decodingError:
            return "–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"
        case .invalidResponse:
            return "–ù–µ–≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞"
        case .serverError(let code):
            return "–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: \(code)"
        case .processingError(let message):
            return "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: \(message)"
        case .networkError(let message):
            return "–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: \(message)"
        }
    }
}