//
//  SpeechRecognitionService.swift
//  MindMap
//
//  Created by Nikita Sergyshkin on 05/08/2025.
//

import Foundation
import Speech
import AVFoundation

// MARK: - Speech Recognition Service
@MainActor
class SpeechRecognitionService: ObservableObject {
    static let shared = SpeechRecognitionService()
    
    private let speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    @Published var isRecognizing = false
    @Published var transcribedText = ""
    
    private init() {
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ru-RU"))
        
        logInfo("üé§ SpeechRecognition –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", category: .speech)
        logInfo("üåç –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: \(speechRecognizer?.isAvailable ?? false)", category: .speech)
    }
    
    // MARK: - Permission Handling
    func requestPermissions() async -> Bool {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
        let speechStatus = await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
        
        guard speechStatus == .authorized else {
            logError("‚ùå –ù–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏: \(speechStatus)", category: .speech)
            return false
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω
        let audioStatus = await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { status in
                continuation.resume(returning: status)
            }
        }
        
        guard audioStatus else {
            logError("‚ùå –ù–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω", category: .speech)
            return false
        }
        
        logInfo("‚úÖ –í—Å–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã", category: .speech)
        return true
    }
    
    // MARK: - Transcribe Audio File
    func transcribeAudioFile(at url: URL) async throws -> String {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            throw SpeechRecognitionError.recognizerUnavailable
        }
        
        logInfo("üéµ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Ñ–∞–π–ª–∞: \(url.lastPathComponent)", category: .speech)
        
        let request = SFSpeechURLRecognitionRequest(url: url)
        request.shouldReportPartialResults = false
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º–∞
        if speechRecognizer.supportsOnDeviceRecognition {
            request.requiresOnDeviceRecognition = true
            logInfo("üîí –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–ª–∞–π–Ω —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", category: .speech)
        } else {
            request.requiresOnDeviceRecognition = false
            logInfo("üåê –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–Ω–ª–∞–π–Ω —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", category: .speech)
        }
        
        return try await withCheckedThrowingContinuation { continuation in
            var hasResumed = false
            
            recognitionTask = speechRecognizer.recognitionTask(with: request) { result, error in
                guard !hasResumed else { return }
                
                if let error = error {
                    let nsError = error as NSError
                    logError("‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: \(error.localizedDescription) (–∫–æ–¥: \(nsError.code))", category: .speech)
                    
                    // –ï—Å–ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è, –ø—Ä–æ–±—É–µ–º –æ–Ω–ª–∞–π–Ω
                    if nsError.domain == "kAFAssistantErrorDomain" && nsError.code == 1101 {
                        logInfo("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –æ–Ω–ª–∞–π–Ω —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", category: .speech)
                        hasResumed = true
                        
                        // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º–∞
                        let onlineRequest = SFSpeechURLRecognitionRequest(url: url)
                        onlineRequest.shouldReportPartialResults = false
                        onlineRequest.requiresOnDeviceRecognition = false
                        
                        speechRecognizer.recognitionTask(with: onlineRequest) { onlineResult, onlineError in
                            if let onlineError = onlineError {
                                continuation.resume(throwing: onlineError)
                                return
                            }
                            
                            if let onlineResult = onlineResult, onlineResult.isFinal {
                                let text = onlineResult.bestTranscription.formattedString
                                logInfo("‚úÖ –û–Ω–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(text.prefix(100))...", category: .speech)
                                continuation.resume(returning: text)
                            }
                        }
                    } else {
                        hasResumed = true
                        continuation.resume(throwing: error)
                    }
                    return
                }
                
                if let result = result, result.isFinal {
                    let text = result.bestTranscription.formattedString
                    logInfo("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(text.prefix(100))...", category: .speech)
                    hasResumed = true
                    continuation.resume(returning: text)
                }
            }
        }
    }
    
    // MARK: - Real-time Recognition
    func startRealTimeRecognition() throws {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            throw SpeechRecognitionError.recognizerUnavailable
        }
        
        if audioEngine.isRunning {
            stopRealTimeRecognition()
        }
        
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw SpeechRecognitionError.requestCreationFailed
        }
        
        recognitionRequest.shouldReportPartialResults = true
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
        if speechRecognizer.supportsOnDeviceRecognition {
            recognitionRequest.requiresOnDeviceRecognition = true
            logInfo("üîí –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º", category: .speech)
        } else {
            recognitionRequest.requiresOnDeviceRecognition = false
            logInfo("üåê –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: –æ–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º", category: .speech)
        }
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        try audioEngine.start()
        
        isRecognizing = true
        transcribedText = ""
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                DispatchQueue.main.async {
                    self.transcribedText = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                logError("‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: \(error.localizedDescription)", category: .speech)
                DispatchQueue.main.async {
                    self.stopRealTimeRecognition()
                }
            }
        }
        
        logInfo("üé§ –ó–∞–ø—É—â–µ–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", category: .speech)
    }
    
    func stopRealTimeRecognition() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        recognitionTask?.cancel()
        recognitionTask = nil
        
        isRecognizing = false
        
        logInfo("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", category: .speech)
    }
    
    // MARK: - Check Availability
    var isOnDeviceRecognitionAvailable: Bool {
        speechRecognizer?.supportsOnDeviceRecognition ?? false
    }
    
    var isSpeechRecognitionAvailable: Bool {
        speechRecognizer?.isAvailable ?? false
    }
}

// MARK: - Errors
enum SpeechRecognitionError: LocalizedError {
    case recognizerUnavailable
    case requestCreationFailed
    case recognitionFailed(String)
    
    var errorDescription: String? {
        switch self {
        case .recognizerUnavailable:
            return "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
        case .requestCreationFailed:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"
        case .recognitionFailed(let message):
            return "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: \(message)"
        }
    }
}

